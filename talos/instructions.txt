chmod +x cloudlab-setup-ubuntu-tl.sh && ./cloudlab-setup-ubuntu-tl.sh && \
sudo apt-get install libvirt-daemon genisoimage libguestfs-tools libosinfo-bin virtinst qemu-kvm git vim net-tools wget curl bash-completion python-pip libvirt-daemon-system virt-manager bridge-utils libnss-libvirt libvirt-clients osinfo-db-tools intltool sshpass p7zip-full p7zip-rar uvtool -y && \
sudo sed -i 's/hosts:          files dns/hosts:          files libvirt libvirt_guest dns/' /etc/nsswitch.conf && sudo lsmod | grep kvm && sudo reboot
#sudo systemctl restart libvirtd && sudo systemctl status libvirtd

screen
# Press Return to continue
# detach from session without killing it: Ctrl a d 
# to see screen sessions: screen -ls
# detach from closed session: screen -d -r 1999.pts-0.node0
# enter session: screen -r 1999.pts-0.node0
# exit a session and terminate it: exit

sudo -i

cd /mnt/extra && cat /sys/module/kvm_intel/parameters/nested && cat /proc/cpuinfo | awk '/^processor/{print $3}' | wc -l && free -h && df -hT && \
sudo virsh list --all && sudo brctl show && wget -O "/mnt/extra/osinfo-db.tar.xz" https://releases.pagure.org/libosinfo/osinfo-db-20250606.tar.xz && \
sudo osinfo-db-import --local "/mnt/extra/osinfo-db.tar.xz"

# Install dependencies
sudo apt update -y && sudo apt-get install apt-transport-https ca-certificates curl gnupg python3-venv -y && \
sudo usermod -aG libvirt `id -un` && sudo adduser `id -un` libvirt-qemu && sudo adduser `id -un` kvm && sudo adduser `id -un` libvirt-dnsmasq && sudo sed -i 's/0770/0777/' /etc/libvirt/libvirtd.conf && \
echo 0 | sudo tee /sys/module/kvm/parameters/halt_poll_ns && echo 'security_driver = "none"' | sudo tee /etc/libvirt/qemu.conf && sudo chmod 0644 /boot/vmlinuz* && \
sudo sed -i -E 's,#?(security_driver)\s*=.*,\1 = "none",g' /etc/libvirt/qemu.conf && \
sudo apt-get install -y docker.io unzip && sudo usermod -aG libvirt $USER && sudo usermod -aG docker $USER && \
virsh pool-define-as default dir --target "/var/lib/libvirt/images" && virsh pool-build default && virsh pool-start default && virsh pool-autostart default && \
mkdir -p /mnt/extra/virt/images && mkdir -p /mnt/extra/virt/vms && sudo systemctl restart libvirtd && sudo systemctl status libvirtd

exit

sudo -i

virsh list --all && virsh net-list --all && virsh pool-list 

##############################################################################################################################################
####################### Talos Linux Kubernetes cluster in libvirt KVM Virtual Machines #################################################
##############################################################################################################################################
###### https://docs.siderolabs.com/talos/v1.12/platform-specific-installations/virtualized-platforms/kvm
###### https://docs.rackspacecloud.com/k8s-talos
###### https://gist.github.com/cyrenity/67469dce33cf4eb4483486637c06d7be
###### https://github.com/joshrnoll/talos-scripts
##############################################################################################################################################
##############################################################################################################################################
### Getting the code
cd /mnt/extra && git clone --recurse-submodules -j4 https://github.com/rackerlabs/genestack /opt/genestack && \
/opt/genestack/bootstrap.sh

###Install kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" && \
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

###Install the convert plugin
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert" && \
sudo install -o root -g root -m 0755 kubectl-convert /usr/local/bin/kubectl-convert

###Install the ko plugin
curl -LO https://raw.githubusercontent.com/kubeovn/kube-ovn/release-1.12/dist/images/kubectl-ko && \
sudo install -o root -g root -m 0755 kubectl-ko /usr/local/bin/kubectl-ko

sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 && sudo chmod a+x /usr/local/bin/yq

curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 && chmod 700 get_helm.sh && ./get_helm.sh

###Install talosctl:
# see https://github.com/siderolabs/talos/releases
# renovate: datasource=github-releases depName=siderolabs/talos
talos_version='1.12.1' && wget https://github.com/siderolabs/talos/releases/download/v$talos_version/talosctl-linux-amd64 && \
sudo install talosctl-linux-amd64 /usr/local/bin/talosctl && rm talosctl-linux-amd64

### Deploy cluster
wget https://factory.talos.dev/image/613e1592b2da41ae5e265e8789429f22e121aab91cb4deb6bc3c0b6262961245/v1.12.1/metal-amd64.iso && \
mv metal-amd64.iso /mnt/extra/virt/images/metal-amd64.iso && \
git clone https://github.com/vpasias/clusterlab.git && cd /mnt/extra/clusterlab/talos && chmod +x ./kvmhost.sh && ./kvmhost.sh

#c0=192.168.254.21 && talosctl get disks --nodes $c0 --insecure && talosctl get links --nodes $c0 --insecure
#c0=192.168.254.21 && talosctl get rd --nodes $c0 --insecure
#rm -rf controlplane.yaml worker.yaml talosconfig

#Configure the Cluster - Add control plane & worker nodes - Set endpoints - Bootstrap the Etcd Cluster
talosctl gen config athena https://192.168.254.21:6443 --install-disk /dev/vda --force && \
talosctl apply-config --file controlplane.yaml --insecure --nodes 192.168.254.21 && \
talosctl apply-config --file controlplane.yaml --insecure --nodes 192.168.254.22 && \
talosctl apply-config --file controlplane.yaml --insecure --nodes 192.168.254.23 && \
talosctl apply-config --file worker.yaml --insecure --nodes 192.168.254.24 && \
talosctl apply-config --file worker.yaml --insecure --nodes 192.168.254.25 && \
talosctl apply-config --file worker.yaml --insecure --nodes 192.168.254.26 && \
talosctl --talosconfig=./talosconfig config endpoints 192.168.254.21 && \
talosctl --talosconfig=./talosconfig config node 192.168.254.21 && \
talosctl bootstrap --nodes 192.168.254.21 --endpoints 192.168.254.21 --talosconfig=./talosconfig

#Show kubernetes information:
export TALOSCONFIG=./talosconfig && talosctl --nodes 192.168.254.21 health && talosctl --nodes 192.168.254.21 kubeconfig $PWD/kubeconfig && \
export KUBECONFIG=$PWD/kubeconfig && kubectl version && kubectl cluster-info && kubectl get nodes -o wide && kubectl get pods -o wide --all-namespaces

###High Availability Control Plane using Talos VIP
#https://jasongodson.com/blog/talos-vip-ha/
for node in 192.168.254.21 192.168.254.22 192.168.254.23; do
  talosctl patch machineconfig --nodes $node --patch @talos-vip-patch.yaml
done

# VIP should respond to pings
ping 192.168.254.254

# Kubernetes API should be accessible
curl -k https://192.168.254.254:6443/version
# Should return: {"kind":"Status",...,"code":401}  # Unauthorized = working!

for node in 192.168.254.21 192.168.254.22 192.168.254.23; do
  talosctl patch machineconfig --nodes $node --patch @cluster-endpoint-patch.yaml
done

# Update kubectl config
kubectl config set-cluster athena --server=https://192.168.254.254:6443

# Test it works
kubectl cluster-info
# Should show Kubernetes control plane is running at https://192.168.254.254:6443

# Update talosctl endpoint
talosctl config endpoint 192.168.254.254

# Most commands work fine
talosctl version

# For health checks, specify --nodes as the VIP
talosctl health --nodes 192.168.254.254

#Testing Failover
# Shutdown one control plane
# Make sure you cordon/drain first if you have things running
#talosctl shutdown --nodes 192.168.254.21
# kubectl should still work immediately!
#kubectl get nodes

###Install Kube OVN
#https://docs.rackspacecloud.com/k8s-cni-kube-ovn/#prerequisites

cat > /etc/genestack/helm-configs/kube-ovn/kube-ovn-helm-overrides.yaml <<EOF
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
---
global:
  registry:
    address: docker.io/kubeovn
    imagePullSecrets: []
networking:
  IFACE: enp1s0
  vlan:
    VLAN_INTERFACE_NAME: enp1s0
OPENVSWITCH_DIR: /var/lib/openvswitch
OVN_DIR: /var/lib/ovn
DISABLE_MODULES_MANAGEMENT: true
EOF

kubectl label node -l beta.kubernetes.io/os=linux kubernetes.io/os=linux
kubectl label node -l node-role.kubernetes.io/control-plane kube-ovn/role=master
kubectl label node -l ovn.kubernetes.io/ovs_dp_type!=userspace ovn.kubernetes.io/ovs_dp_type=kernel

/opt/genestack/bin/install-kube-ovn.sh

kubectl get subnets.kubeovn.io

###Deploy the Rook operator

kubectl apply -k /etc/genestack/kustomize/rook-operator/base

kubectl label node node4 role=storage-node && kubectl label node node5 role=storage-node && \
kubectl label node node6 role=storage-node

cat > /etc/genestack/kustomize/rook-operator/base/talos.yaml <<EOF
apiVersion: v1
kind: Namespace
metadata:
  labels:
    kubernetes.io/metadata.name: rook-ceph
    pod-security.kubernetes.io/audit: privileged
    pod-security.kubernetes.io/audit-version: latest
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/enforce-version: latest
    pod-security.kubernetes.io/warn: privileged
    pod-security.kubernetes.io/warn-version: latest
  name: rook-ceph
EOF

kubectl apply -f /etc/genestack/kustomize/rook-operator/base/talos.yaml

#Deploy the Rook cluster
kubectl apply -k /etc/genestack/kustomize/rook-cluster/overlay

#Validate the cluster is operational
kubectl --namespace rook-ceph get cephclusters.ceph.rook.io

#Create Storage Classes
kubectl apply -k /etc/genestack/kustomize/rook-defaults/base

kubectl -n rook-ceph patch CephCluster rook-ceph  --type=merge -p "{\"spec\": {\"monitoring\": {\"enabled\": true}}}"

###Label all of the nodes in the environment

# Label the openstack controllers
kubectl label node $(kubectl get nodes | awk '/control-plane/ {print $1}') openstack-control-plane=enabled

# Label the openstack compute nodes
kubectl label node node4 openstack-compute-node=enabled && kubectl label node node5 openstack-compute-node=enabled && \
kubectl label node node6 openstack-compute-node=enabled

# Label the openstack network nodes
kubectl label node $(kubectl get nodes | awk '/control-plane/ {print $1}') openstack-network-node=enabled

# Label the openstack storage nodes
kubectl label node node4 openstack-storage-node=enabled && kubectl label node node5 openstack-storage-node=enabled && \
kubectl label node node6 openstack-storage-node=enabled

# Label all workers - Recommended and used when deploying Kubernetes specific services
kubectl label node node4 node-role.kubernetes.io/worker=worker && kubectl label node node5 node-role.kubernetes.io/worker=worker && \
kubectl label node node6 node-role.kubernetes.io/worker=worker

# With OVN we need the compute nodes to be "network" nodes as well. While they will be configured for networking, they wont be gateways.
kubectl label node $(kubectl get nodes | awk '/worker/ {print $1}') openstack-network-node=enabled

# Verify the nodes are operational and labeled.
kubectl get nodes -o json | jq '[.items[] | {"NAME": .metadata.name, "LABELS": .metadata.labels}]'

###Remove taint from control-plane nodes
kubectl taint nodes -l node-role.kubernetes.io/control-plane node-role.kubernetes.io/control-plane:NoSchedule-

###Install the Prometheus Stack
/opt/genestack/bin/install-kube-prometheus-stack.sh

kubectl -n prometheus get pods -l "release=kube-prometheus-stack"

##############################################################################################################################################
### Troubleshoot ###
##############################################################################################################################################

#Talos:
# see https://www.talos.dev/v1.2/advanced/troubleshooting-control-plane/

talosctl -n $c0 service etcd status
talosctl -n $c0 etcd members
talosctl -n $c0 get members
talosctl -n $c0 health --control-plane-nodes $controllers --worker-nodes $workers
talosctl -n $c0 dashboard
talosctl -n $c0 logs controller-runtime
talosctl -n $c0 logs kubelet
talosctl -n $c0 disks
talosctl -n $c0 mounts | sort
talosctl -n $c0 get resourcedefinitions
talosctl -n $c0 get machineconfigs -o yaml
talosctl -n $c0 get staticpods -o yaml
talosctl -n $c0 get staticpodstatus
talosctl -n $c0 get manifests
talosctl -n $c0 get services
talosctl -n $c0 get extensions
talosctl -n $c0 get addresses
talosctl -n $c0 get nodeaddresses
talosctl -n $c0 list -l -r -t f /etc
talosctl -n $c0 list -l -r -t f /system
talosctl -n $c0 list -l -r -t f /var
talosctl -n $c0 list -l /sys/fs/cgroup
talosctl -n $c0 read /proc/cmdline | tr ' ' '\n'
talosctl -n $c0 read /proc/mounts | sort
talosctl -n $c0 read /etc/resolv.conf
talosctl -n $c0 read /etc/containerd/config.toml
talosctl -n $c0 read /etc/cri/containerd.toml
talosctl -n $c0 read /etc/cri/conf.d/cri.toml
talosctl -n $c0 read /etc/kubernetes/kubelet.yaml
talosctl -n $c0 read /etc/kubernetes/bootstrap-kubeconfig
talosctl -n $c0 ps
talosctl -n $c0 containers -k

#Kubernetes:

kubectl get events --all-namespaces --watch
kubectl --namespace kube-system get events --watch
kubectl run busybox -it --rm --restart=Never --image=busybox:1.36 -- nslookup -type=a talos.dev

##############################################################################################################################################
### Delete Infrastructure ###
##############################################################################################################################################

#for i in {1..6}; do virsh shutdown node$i; done && sleep 10 && virsh list --all && for i in {1..3}; do virsh start node$i; done && sleep 10 && virsh list --all

cd /mnt/extra/ && for i in {1..6}; do virsh shutdown node$i; done && sleep 30 && for i in {1..6}; do virsh destroy node$i; done && sleep 30 && \
for i in {1..6}; do virsh undefine node$i --remove-all-storage; done && sleep 10 && rm -rf /mnt/extra/vbdnode1* && virsh net-destroy management && \
rm -rf /mnt/extra/management.xml && virsh net-destroy service && rm -rf /mnt/extra/service.xml && virsh net-undefine management && virsh net-undefine service && \
rm -rf /root/.ssh/known_hosts && touch /root/.ssh/known_hosts && sudo virsh list --all && sudo brctl show && sudo virsh net-list --all && rm -rf /mnt/extra/clusterlab 
