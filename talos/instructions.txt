chmod +x cloudlab-setup-ubuntu-tl.sh && ./cloudlab-setup-ubuntu-tl.sh && \
sudo apt-get install libvirt-daemon genisoimage libguestfs-tools libosinfo-bin virtinst qemu-kvm git vim net-tools wget curl bash-completion python-pip libvirt-daemon-system virt-manager bridge-utils libnss-libvirt libvirt-clients osinfo-db-tools intltool sshpass p7zip-full p7zip-rar uvtool -y && \
sudo sed -i 's/hosts:          files dns/hosts:          files libvirt libvirt_guest dns/' /etc/nsswitch.conf && sudo lsmod | grep kvm && sudo reboot
#sudo systemctl restart libvirtd && sudo systemctl status libvirtd

screen
# Press Return to continue
# detach from session without killing it: Ctrl a d 
# to see screen sessions: screen -ls
# detach from closed session: screen -d -r 2014.pts-0.node0
# enter session: screen -r 2014.pts-0.node0
# exit a session and terminate it: exit

sudo -i

cd /mnt/extra && cat /sys/module/kvm_intel/parameters/nested && cat /proc/cpuinfo | awk '/^processor/{print $3}' | wc -l && free -h && df -hT && sudo virsh list --all && sudo brctl show && \
wget -O "/mnt/extra/osinfo-db.tar.xz" https://releases.pagure.org/libosinfo/osinfo-db-20250606.tar.xz && sudo osinfo-db-import --local "/mnt/extra/osinfo-db.tar.xz"

# Install dependencies
sudo apt update -y && sudo apt-get install apt-transport-https ca-certificates curl gnupg python3-venv -y && \
sudo usermod -aG libvirt `id -un` && sudo adduser `id -un` libvirt-qemu && sudo adduser `id -un` kvm && sudo adduser `id -un` libvirt-dnsmasq && sudo sed -i 's/0770/0777/' /etc/libvirt/libvirtd.conf && \
echo 0 | sudo tee /sys/module/kvm/parameters/halt_poll_ns && echo 'security_driver = "none"' | sudo tee /etc/libvirt/qemu.conf && sudo chmod 0644 /boot/vmlinuz* && \
sudo sed -i -E 's,#?(security_driver)\s*=.*,\1 = "none",g' /etc/libvirt/qemu.conf && \
sudo systemctl restart libvirtd && sudo systemctl status libvirtd && \
sudo apt-get install -y docker.io unzip && \
sudo usermod -aG libvirt $USER && sudo usermod -aG docker $USER && \
virsh pool-define-as default dir --target "/var/lib/libvirt/images" && virsh pool-build default && virsh pool-start default && virsh pool-autostart default && \
mkdir -p /mnt/extra/virt/images && mkdir -p /mnt/extra/virt/vms

exit

sudo -i

virsh list --all && virsh net-list --all && virsh pool-list 

##############################################################################################################################################
####################### Talos Linux Kubernetes cluster in libvirt KVM Virtual Machines #################################################
##############################################################################################################################################
###### https://docs.siderolabs.com/talos/v1.12/platform-specific-installations/virtualized-platforms/kvm
###### https://gist.github.com/cyrenity/67469dce33cf4eb4483486637c06d7be
###### https://github.com/joshrnoll/talos-scripts
##############################################################################################################################################
##############################################################################################################################################
# Getting the code
cd /mnt/extra && git clone --recurse-submodules -j4 https://github.com/rackerlabs/genestack /opt/genestack && \
/opt/genestack/bootstrap.sh

#Install kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" && \
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

#Install the convert plugin
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert" && \
sudo install -o root -g root -m 0755 kubectl-convert /usr/local/bin/kubectl-convert

#Install the ko plugin
curl -LO https://raw.githubusercontent.com/kubeovn/kube-ovn/release-1.12/dist/images/kubectl-ko && \
sudo install -o root -g root -m 0755 kubectl-ko /usr/local/bin/kubectl-ko

#Install talosctl:
# see https://github.com/siderolabs/talos/releases
# renovate: datasource=github-releases depName=siderolabs/talos
talos_version='1.12.1' && git clone https://github.com/vpasias/clusterlab.git && cd /mnt/extra/clusterlab/talos && \
wget https://github.com/siderolabs/talos/releases/download/v$talos_version/talosctl-linux-amd64 && \
sudo install talosctl-linux-amd64 /usr/local/bin/talosctl && rm talosctl-linux-amd64 && \
wget https://github.com/siderolabs/talos/releases/download/v$talos_version/metal-amd64.iso && mv metal-amd64.iso /mnt/extra/virt/images/metal-amd64.iso && \
chmod +x ./kvmhost.sh && ./kvmhost.sh

c0=192.168.254.21 && talosctl get disks --nodes $c0 --insecure

MY_VIP=192.168.254.254 && cp EXAMPLE-network-config.yaml network-config.yaml && sed -i -e s/placeholder/$MY_VIP/ network-config.yaml && \
chmod +x ./gen-cluster.sh && ./gen-cluster.sh
#CLUSTER_NAME=athena
#NODE_IP=192.168.254.21
#VIP=192.168.254.254

#Once the cluster is bootstrapped, add additional control plane nodes:
talosctl apply-config --file controlplane.yaml --insecure --nodes 192.168.254.22
talosctl apply-config --file controlplane.yaml --insecure --nodes 192.168.254.23

#Add worker nodes:
talosctl apply-config --file worker.yaml --insecure --nodes 192.168.254.24
talosctl apply-config --file worker.yaml --insecure --nodes 192.168.254.25
talosctl apply-config --file worker.yaml --insecure --nodes 192.168.254.26

#Show kubernetes information:
talosctl -n 192.168.254.21 kubeconfig $PWD/kubeconfig && \
export KUBECONFIG=$PWD/kubeconfig && kubectl version && kubectl cluster-info && kubectl get nodes -o wide && kubectl get pods -o wide --all-namespaces

#Install Kube OVN
#https://docs.rackspacecloud.com/k8s-cni-kube-ovn/#prerequisites

cat > /etc/genestack/helm-configs/kube-ovn/kube-ovn-helm-overrides.yaml <<EOF
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
---
global:
  registry:
    address: docker.io/kubeovn
    imagePullSecrets: []
networking:
  IFACE: eth0
  vlan:
    VLAN_INTERFACE_NAME: eth0
OPENVSWITCH_DIR: /var/lib/openvswitch
OVN_DIR: /var/lib/ovn
DISABLE_MODULES_MANAGEMENT: true
EOF

kubectl label node -l beta.kubernetes.io/os=linux kubernetes.io/os=linux
kubectl label node -l node-role.kubernetes.io/control-plane kube-ovn/role=master
kubectl label node -l ovn.kubernetes.io/ovs_dp_type!=userspace ovn.kubernetes.io/ovs_dp_type=kernel

/opt/genestack/bin/install-kube-ovn.sh

kubectl get subnets.kubeovn.io

#Label all of the nodes in the environment

# Label the openstack controllers
kubectl label node $(kubectl get nodes | awk '/controller/ {print $1}') openstack-control-plane=enabled

# Label the openstack compute nodes
kubectl label node $(kubectl get nodes | awk '/compute/ {print $1}') openstack-compute-node=enabled

# Label the openstack network nodes
kubectl label node $(kubectl get nodes | awk '/network/ {print $1}') openstack-network-node=enabled

# Label the openstack storage nodes
kubectl label node $(kubectl get nodes | awk '/storage/ {print $1}') openstack-storage-node=enabled

# With OVN we need the compute nodes to be "network" nodes as well. While they will be configured for networking, they wont be gateways.
kubectl label node $(kubectl get nodes | awk '/compute/ {print $1}') openstack-network-node=enabled

# Label all workers - Recommended and used when deploying Kubernetes specific services
kubectl label node $(kubectl get nodes | awk '/worker/ {print $1}')  node-role.kubernetes.io/worker=worker

# Verify the nodes are operational and labeled.
kubectl get nodes -o json | jq '[.items[] | {"NAME": .metadata.name, "LABELS": .metadata.labels}]'

# Remote taint from control-plane nodes
kubectl taint nodes -l node-role.kubernetes.io/control-plane node-role.kubernetes.io/control-plane:NoSchedule-

#Install the Prometheus Stack
/opt/genestack/bin/install-kube-prometheus-stack.sh

kubectl -n prometheus get pods -l "release=kube-prometheus-stack"

#Deploy the Rook operator

kubectl apply -k /etc/genestack/kustomize/rook-operator/base

kubectl label node node1 role=storage-node && kubectl label node node2 role=storage-node && \
kubectl label node node3 role=storage-node

cat > /etc/genestack/kustomize/rook-operator/base/talos.yaml <<EOF
apiVersion: v1
kind: Namespace
metadata:
  labels:
    kubernetes.io/metadata.name: rook-ceph
    pod-security.kubernetes.io/audit: privileged
    pod-security.kubernetes.io/audit-version: latest
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/enforce-version: latest
    pod-security.kubernetes.io/warn: privileged
    pod-security.kubernetes.io/warn-version: latest
  name: rook-ceph
EOF

kubectl apply -k /etc/genestack/kustomize/rook-operator/base/talos.yaml

#Deploy the Rook cluster
kubectl apply -k /etc/genestack/kustomize/rook-cluster/overlay

#Validate the cluster is operational
kubectl --namespace rook-ceph get cephclusters.ceph.rook.io

#Create Storage Classes
kubectl apply -k /etc/genestack/kustomize/rook-defaults/base

kubectl -n rook-ceph patch CephCluster rook-ceph  --type=merge -p "{\"spec\": {\"monitoring\": {\"enabled\": true}}}"

##############################################################################################################################################
### Troubleshoot ###
##############################################################################################################################################

#Talos:
# see https://www.talos.dev/v1.2/advanced/troubleshooting-control-plane/

talosctl -n $c0 service etcd status
talosctl -n $c0 etcd members
talosctl -n $c0 get members
talosctl -n $c0 health --control-plane-nodes $controllers --worker-nodes $workers
talosctl -n $c0 dashboard
talosctl -n $c0 logs controller-runtime
talosctl -n $c0 logs kubelet
talosctl -n $c0 disks
talosctl -n $c0 mounts | sort
talosctl -n $c0 get resourcedefinitions
talosctl -n $c0 get machineconfigs -o yaml
talosctl -n $c0 get staticpods -o yaml
talosctl -n $c0 get staticpodstatus
talosctl -n $c0 get manifests
talosctl -n $c0 get services
talosctl -n $c0 get extensions
talosctl -n $c0 get addresses
talosctl -n $c0 get nodeaddresses
talosctl -n $c0 list -l -r -t f /etc
talosctl -n $c0 list -l -r -t f /system
talosctl -n $c0 list -l -r -t f /var
talosctl -n $c0 list -l /sys/fs/cgroup
talosctl -n $c0 read /proc/cmdline | tr ' ' '\n'
talosctl -n $c0 read /proc/mounts | sort
talosctl -n $c0 read /etc/resolv.conf
talosctl -n $c0 read /etc/containerd/config.toml
talosctl -n $c0 read /etc/cri/containerd.toml
talosctl -n $c0 read /etc/cri/conf.d/cri.toml
talosctl -n $c0 read /etc/kubernetes/kubelet.yaml
talosctl -n $c0 read /etc/kubernetes/bootstrap-kubeconfig
talosctl -n $c0 ps
talosctl -n $c0 containers -k

#Kubernetes:

kubectl get events --all-namespaces --watch
kubectl --namespace kube-system get events --watch
kubectl run busybox -it --rm --restart=Never --image=busybox:1.36 -- nslookup -type=a talos.dev

##############################################################################################################################################
### Delete Infrastructure ###
##############################################################################################################################################

#for i in {1..6}; do virsh shutdown node$i; done && sleep 10 && virsh list --all && for i in {1..3}; do virsh start node$i; done && sleep 10 && virsh list --all

cd /mnt/extra/ && for i in {1..6}; do virsh shutdown node$i; done && sleep 10 && for i in {1..6}; do virsh destroy node$i; done && sleep 10 && \
for i in {1..6}; do virsh undefine node$i --remove-all-storage; done && sleep 10 && rm -rf vbdnode1* && virsh net-destroy management && \
rm -rf /mnt/extra/management.xml && virsh net-destroy service && rm -rf /mnt/extra/service.xml && virsh net-undefine management && virsh net-undefine service && \
rm -rf /root/.ssh/known_hosts && touch /root/.ssh/known_hosts && sudo virsh list --all && sudo brctl show && sudo virsh net-list --all && rm -rf /mnt/extra/clusterlab 
